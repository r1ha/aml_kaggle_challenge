{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117959,"databundleVersionId":14220991,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install info-nce-pytorch\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom tqdm.auto import tqdm\n# from info_nce import InfoNCE, info_nce\n\nclass TextImageEmbeddingDataset(Dataset):\n    def __init__(self, text_embeddings, image_embeddings):\n        \"\"\"\n        Args:\n            text_embeddings: torch.Tensor (N, 1024)\n            image_embeddings: torch.Tensor (N, 1536)\n        \"\"\"\n        self.text_embeddings = text_embeddings\n        self.image_embeddings = image_embeddings\n\n    def __len__(self):\n        return len(self.text_embeddings)\n\n    def __getitem__(self, idx):\n        text_emb = self.text_embeddings[idx]\n        img_emb = self.image_embeddings[idx]\n        return text_emb, img_emb\n\n\ndef create_dataloader(text_tensor, image_tensor, batch_size=64, shuffle=True, num_workers=0):\n    dataset = TextImageEmbeddingDataset(text_tensor, image_tensor)\n    loader = DataLoader(\n        dataset, \n        batch_size=batch_size, \n        shuffle=shuffle, \n        num_workers=num_workers\n    )\n    return loader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-12T09:54:42.585581Z","iopub.execute_input":"2025-11-12T09:54:42.585871Z","iopub.status.idle":"2025-11-12T09:54:42.591954Z","shell.execute_reply.started":"2025-11-12T09:54:42.585848Z","shell.execute_reply":"2025-11-12T09:54:42.591230Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"print(\"=== DATA LOADING & SETUP ===\")\n\nBATCH_SIZE = 256\nVAL_SPLIT = 0.1\nRANDOM_STATE = 11\n\ndata = np.load(\"/kaggle/input/aml-competition/train/train/train.npz\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Apply normalization on embeddings before training.\ntext_embeddings_cpu = F.normalize(torch.tensor(data[\"captions/embeddings\"], dtype=torch.float32), dim=1)\nimage_embeddings_small_cpu = F.normalize(torch.tensor(data[\"images/embeddings\"], dtype=torch.float32), dim=1)\nimage_embeddings_cpu = image_embeddings_small_cpu.repeat_interleave(5, dim=0)\n\n# Create train/validation split\nindices = np.arange(len(text_embeddings_cpu))\ntrain_indices, val_indices = train_test_split(\n    indices, \n    test_size=VAL_SPLIT, \n    random_state=RANDOM_STATE\n)\n\ntext_train = text_embeddings_cpu[train_indices]\nimg_train = image_embeddings_cpu[train_indices]\n\ntext_val = text_embeddings_cpu[val_indices]\nimg_val = image_embeddings_cpu[val_indices]\n\ntrain_loader = create_dataloader(text_train, img_train, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = create_dataloader(text_val, img_val, batch_size=BATCH_SIZE*2, shuffle=False)\n\nprint(f\"Training samples: {len(text_train)}, Validation samples: {len(text_val)}\")\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB)\")\nelse:\n    print(\"CUDA not available. Training will be slow.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T09:54:44.908214Z","iopub.execute_input":"2025-11-12T09:54:44.908481Z","iopub.status.idle":"2025-11-12T09:54:55.411530Z","shell.execute_reply.started":"2025-11-12T09:54:44.908461Z","shell.execute_reply":"2025-11-12T09:54:55.410704Z"}},"outputs":[{"name":"stdout","text":"=== DATA LOADING & SETUP ===\nTraining samples: 112500, Validation samples: 12500\nUsing device: cuda\nGPU: Tesla T4 (15.8 GB)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class ResidualMLP(nn.Module):\n    def __init__(self, input_dim=1024, hidden_dim=2048, output_dim=1536, dropout=0.2, use_ln=True):\n        super().__init__()\n                \n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.norm1 = nn.LayerNorm(hidden_dim)\n        \n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.norm2 = nn.LayerNorm(output_dim)\n        \n        self.act = nn.GELU()\n        self.dropout = nn.Dropout(dropout)\n        \n        # Residual projection (only if dimensions differ)\n        self.projection = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        identity = self.projection(x)\n\n        out = self.fc1(x)\n        out = self.norm1(out)\n        out = self.act(out)\n        out = self.dropout(out)\n        \n        out = self.fc2(out)\n        out = self.norm2(out)\n        \n        out = out + identity\n        out = self.act(out)\n        return out\n\nclass InfoNCELoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.criterion = nn.CrossEntropyLoss()\n\n    def forward(self, pred_embeds, target_embeds):\n        # Normalize embeddings\n        pred_embeds = F.normalize(pred_embeds, dim=1)\n        target_embeds = F.normalize(target_embeds, dim=1)\n\n        # Calculate logits (N, N) matrix\n        # (pred @ target.T) is the matrix of all-to-all cosine similarities\n        logits = (pred_embeds @ target_embeds.T) / self.temperature\n        \n        # Labels: diagonal elements (i, i) are the positive pairs\n        batch_size = pred_embeds.shape[0]\n        labels = torch.arange(batch_size, device=pred_embeds.device)\n        \n        return F.cross_entropy(logits, labels)\n\nmlp = ResidualMLP().to(device)\nloss_function = InfoNCELoss().to(device)\n# loss_function = InfoNCE().to(device)\noptimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n# Mixed Precision for speeding up GPU training\nscaler = torch.amp.GradScaler(device_str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T11:14:22.938476Z","iopub.execute_input":"2025-11-12T11:14:22.939175Z","iopub.status.idle":"2025-11-12T11:14:23.010863Z","shell.execute_reply.started":"2025-11-12T11:14:22.939150Z","shell.execute_reply":"2025-11-12T11:14:23.009954Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"num_epochs = 100\npatience = 3\nbest_val_loss = float('inf')\npatience_counter = 0\n\nnum_train_steps = num_epochs * len(train_loader)\nscheduler = CosineAnnealingLR(optimizer, T_max=num_train_steps)\n\nprint(f\"Starting training for max {num_epochs} epochs...\")\nprint(f\"Batch size: {BATCH_SIZE}. Optimizer: Adam. Loss: InfoNCE.\")\nprint(f\"Features: Mixed Precision, Cosine LR Scheduler, Early Stopping (patience={patience}).\")\n\nfor epoch in range(num_epochs):\n    # --- Training Phase ---\n    mlp.train()\n    epoch_train_losses = []\n    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n    \n    for text_batch, image_batch in train_loop:\n        text_batch = text_batch.to(device)\n        image_batch = image_batch.to(device)\n        \n        optimizer.zero_grad()\n        \n        with torch.amp.autocast(device_str, dtype=torch.float16):\n            output = mlp(text_batch)\n            loss = loss_function(output, image_batch)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        scheduler.step()\n        \n        epoch_train_losses.append(loss.item())\n        train_loop.set_postfix(loss=np.mean(epoch_train_losses[-20:])) # Moving avg loss\n        \n    avg_train_loss = np.mean(epoch_train_losses)\n\n    # --- Validation Phase ---\n    mlp.eval()\n    epoch_val_losses = []\n    val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n    \n    with torch.no_grad():\n        for text_batch, image_batch in val_loop:\n            text_batch = text_batch.to(device)\n            image_batch = image_batch.to(device)\n            \n            with torch.amp.autocast(device_str, dtype=torch.float16):\n                output = mlp(text_batch)\n                loss = loss_function(output, image_batch)\n                \n            epoch_val_losses.append(loss.item())\n            val_loop.set_postfix(val_loss=np.mean(epoch_val_losses))\n\n    avg_val_loss = np.mean(epoch_val_losses)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n\n    # --- Early Stopping & Model Checkpointing ---\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(mlp.state_dict(), 'pipeline_best_model.pth')\n        print(f\"âœ… New best model saved! Val Loss: {best_val_loss:.6f}\")\n    else:\n        patience_counter += 1\n        print(f\"âš ï¸ No improvement. Patience: {patience_counter}/{patience}\")\n\n    if patience_counter >= patience:\n        print(f\"ðŸ›‘ Early stopping triggered after {epoch+1} epochs.\")\n        break\n\nprint(\"\\n Training complete.\")\nprint(f\"Loading best model from checkpoint (Val Loss: {best_val_loss:.6f})...\")\n\n# Load the best model state for inference\nmlp.load_state_dict(torch.load('pipeline_best_model.pth'))\nprint(\"Best model loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T11:14:25.643173Z","iopub.execute_input":"2025-11-12T11:14:25.643684Z","iopub.status.idle":"2025-11-12T11:17:32.515611Z","shell.execute_reply.started":"2025-11-12T11:14:25.643662Z","shell.execute_reply":"2025-11-12T11:17:32.515060Z"}},"outputs":[{"name":"stdout","text":"Starting training for max 100 epochs...\nBatch size: 256. Optimizer: AdamW. Loss: InfoNCE.\nFeatures: Mixed Precision, Cosine LR Scheduler, Early Stopping (patience=3).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1/100 - Train Loss: 3.461193 - Val Loss: 3.638513\nâœ… New best model saved! Val Loss: 3.638513\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/100 - Train Loss: 2.926485 - Val Loss: 3.411460\nâœ… New best model saved! Val Loss: 3.411460\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3/100 - Train Loss: 2.733214 - Val Loss: 3.272392\nâœ… New best model saved! Val Loss: 3.272392\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 4/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/100 - Train Loss: 2.597917 - Val Loss: 3.171453\nâœ… New best model saved! Val Loss: 3.171453\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 5/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/100 - Train Loss: 2.493502 - Val Loss: 3.088863\nâœ… New best model saved! Val Loss: 3.088863\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 6/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/100 - Train Loss: 2.409382 - Val Loss: 3.018749\nâœ… New best model saved! Val Loss: 3.018749\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 7/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/100 - Train Loss: 2.339213 - Val Loss: 2.975388\nâœ… New best model saved! Val Loss: 2.975388\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 8/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8/100 - Train Loss: 2.278151 - Val Loss: 2.929510\nâœ… New best model saved! Val Loss: 2.929510\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 9/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9/100 - Train Loss: 2.225966 - Val Loss: 2.895593\nâœ… New best model saved! Val Loss: 2.895593\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 10/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/100 - Train Loss: 2.177158 - Val Loss: 2.857734\nâœ… New best model saved! Val Loss: 2.857734\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 11/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11/100 - Train Loss: 2.136036 - Val Loss: 2.831756\nâœ… New best model saved! Val Loss: 2.831756\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 12/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12/100 - Train Loss: 2.098324 - Val Loss: 2.808294\nâœ… New best model saved! Val Loss: 2.808294\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 13/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13/100 - Train Loss: 2.065810 - Val Loss: 2.798307\nâœ… New best model saved! Val Loss: 2.798307\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 14/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14/100 - Train Loss: 2.032909 - Val Loss: 2.775391\nâœ… New best model saved! Val Loss: 2.775391\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 15/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15/100 - Train Loss: 2.003089 - Val Loss: 2.755266\nâœ… New best model saved! Val Loss: 2.755266\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 16/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 16/100 - Train Loss: 1.975106 - Val Loss: 2.743573\nâœ… New best model saved! Val Loss: 2.743573\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 17/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 17/100 - Train Loss: 1.949145 - Val Loss: 2.728972\nâœ… New best model saved! Val Loss: 2.728972\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 18/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 18/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 18/100 - Train Loss: 1.925747 - Val Loss: 2.724041\nâœ… New best model saved! Val Loss: 2.724041\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 19/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 19/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 19/100 - Train Loss: 1.901724 - Val Loss: 2.708208\nâœ… New best model saved! Val Loss: 2.708208\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 20/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 20/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 20/100 - Train Loss: 1.882936 - Val Loss: 2.703723\nâœ… New best model saved! Val Loss: 2.703723\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 21/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 21/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 21/100 - Train Loss: 1.862616 - Val Loss: 2.688456\nâœ… New best model saved! Val Loss: 2.688456\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 22/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 22/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 22/100 - Train Loss: 1.843903 - Val Loss: 2.693076\nâš ï¸ No improvement. Patience: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 23/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 23/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 23/100 - Train Loss: 1.825021 - Val Loss: 2.675665\nâœ… New best model saved! Val Loss: 2.675665\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 24/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 24/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 24/100 - Train Loss: 1.809418 - Val Loss: 2.671911\nâœ… New best model saved! Val Loss: 2.671911\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 25/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 25/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 25/100 - Train Loss: 1.792582 - Val Loss: 2.663079\nâœ… New best model saved! Val Loss: 2.663079\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 26/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 26/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 26/100 - Train Loss: 1.776602 - Val Loss: 2.670077\nâš ï¸ No improvement. Patience: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 27/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 27/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 27/100 - Train Loss: 1.760311 - Val Loss: 2.656398\nâœ… New best model saved! Val Loss: 2.656398\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 28/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 28/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 28/100 - Train Loss: 1.747103 - Val Loss: 2.643148\nâœ… New best model saved! Val Loss: 2.643148\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 29/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 29/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 29/100 - Train Loss: 1.735879 - Val Loss: 2.645500\nâš ï¸ No improvement. Patience: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 30/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 30/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 30/100 - Train Loss: 1.721212 - Val Loss: 2.639600\nâœ… New best model saved! Val Loss: 2.639600\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 31/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 31/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 31/100 - Train Loss: 1.705186 - Val Loss: 2.636943\nâœ… New best model saved! Val Loss: 2.636943\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 32/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 32/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 32/100 - Train Loss: 1.695019 - Val Loss: 2.640312\nâš ï¸ No improvement. Patience: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 33/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 33/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 33/100 - Train Loss: 1.684423 - Val Loss: 2.625415\nâœ… New best model saved! Val Loss: 2.625415\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 34/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 34/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 34/100 - Train Loss: 1.670056 - Val Loss: 2.635192\nâš ï¸ No improvement. Patience: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 35/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 35/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 35/100 - Train Loss: 1.659860 - Val Loss: 2.624710\nâœ… New best model saved! Val Loss: 2.624710\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 36/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 36/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 36/100 - Train Loss: 1.647867 - Val Loss: 2.622309\nâœ… New best model saved! Val Loss: 2.622309\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 37/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 37/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 37/100 - Train Loss: 1.640387 - Val Loss: 2.626363\nâš ï¸ No improvement. Patience: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 38/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 38/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 38/100 - Train Loss: 1.628394 - Val Loss: 2.616800\nâœ… New best model saved! Val Loss: 2.616800\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 39/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 39/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 39/100 - Train Loss: 1.616963 - Val Loss: 2.616823\nâš ï¸ No improvement. Patience: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 40/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 40/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 40/100 - Train Loss: 1.608209 - Val Loss: 2.610139\nâœ… New best model saved! Val Loss: 2.610139\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 41/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 41/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 41/100 - Train Loss: 1.601002 - Val Loss: 2.617403\nâš ï¸ No improvement. Patience: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 42/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 42/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 42/100 - Train Loss: 1.593979 - Val Loss: 2.614320\nâš ï¸ No improvement. Patience: 2/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 43/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 43/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 43/100 - Train Loss: 1.582668 - Val Loss: 2.609632\nâœ… New best model saved! Val Loss: 2.609632\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 44/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 44/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 44/100 - Train Loss: 1.574286 - Val Loss: 2.613207\nâš ï¸ No improvement. Patience: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 45/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 45/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 45/100 - Train Loss: 1.566293 - Val Loss: 2.603633\nâœ… New best model saved! Val Loss: 2.603633\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 46/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 46/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 46/100 - Train Loss: 1.557138 - Val Loss: 2.606899\nâš ï¸ No improvement. Patience: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 47/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 47/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 47/100 - Train Loss: 1.548444 - Val Loss: 2.606104\nâš ï¸ No improvement. Patience: 2/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 48/100 [Train]:   0%|          | 0/440 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 48/100 [Val]:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 48/100 - Train Loss: 1.545602 - Val Loss: 2.606025\nâš ï¸ No improvement. Patience: 3/3\nðŸ›‘ Early stopping triggered after 48 epochs.\n\nâœ… Training complete.\nLoading best model from checkpoint (Val Loss: 2.603633)...\nBest model loaded.\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"# Evaluate model on training data\n\nmlp.eval()\neval_losses = []\nwith torch.no_grad():\n    for i, (text_batch, image_batch) in enumerate(train_loader):\n        if i >= 5:\n            break\n        text_batch = text_batch.to(device)\n        image_batch = image_batch.to(device)\n        output = mlp(text_batch)\n        loss = loss_function(output, image_batch)\n        eval_losses.append(loss.item())\n\nprint(f\"Training Eval Avg Loss: {np.mean(eval_losses):.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T11:17:44.933917Z","iopub.execute_input":"2025-11-12T11:17:44.934208Z","iopub.status.idle":"2025-11-12T11:17:44.972920Z","shell.execute_reply.started":"2025-11-12T11:17:44.934189Z","shell.execute_reply":"2025-11-12T11:17:44.972331Z"}},"outputs":[{"name":"stdout","text":"Training Eval Avg Loss: 1.322532\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"# Load test data\n\ntest_data = np.load(\"/kaggle/input/aml-competition/test/test/test.clean.npz\")\n\ncaption_ids = test_data[\"captions/ids\"]\ncaption_texts = test_data[\"captions/text\"]\n\ntest_text_embeddings_raw = torch.tensor(test_data[\"captions/embeddings\"], dtype=torch.float32)\n\n# Normalize test embeddings\ntest_text_embeddings = F.normalize(test_text_embeddings_raw, dim=1).to(device)\n\nprint(f\"Loaded and normalized {len(caption_ids)} test captions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T11:17:47.038166Z","iopub.execute_input":"2025-11-12T11:17:47.038654Z","iopub.status.idle":"2025-11-12T11:17:47.093935Z","shell.execute_reply.started":"2025-11-12T11:17:47.038630Z","shell.execute_reply":"2025-11-12T11:17:47.093248Z"}},"outputs":[{"name":"stdout","text":"\n=== TESTING PHASE ===\nLoaded and normalized 1500 test captions.\n","output_type":"stream"}],"execution_count":102},{"cell_type":"code","source":"# Generate predictions\n\npredictions = []\nbatch_size = 256\nmlp.eval()\n\nprint(\"Generating predictions...\")\nwith torch.no_grad():\n    for i in tqdm(range(0, len(test_text_embeddings), batch_size), desc=\"Predicting\"):\n        batch = test_text_embeddings[i:i+batch_size]\n        \n        with torch.amp.autocast(device_str, dtype=torch.float16):\n            predicted_batch = mlp(batch)\n            \n        predictions.append(predicted_batch.float().cpu().numpy())\n\npredictions = np.concatenate(predictions, axis=0)\nprint(f\"Generated embeddings for {len(predictions)} captions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T11:17:47.471344Z","iopub.execute_input":"2025-11-12T11:17:47.471570Z","iopub.status.idle":"2025-11-12T11:17:47.497760Z","shell.execute_reply.started":"2025-11-12T11:17:47.471552Z","shell.execute_reply":"2025-11-12T11:17:47.497217Z"}},"outputs":[{"name":"stdout","text":"Generating predictions...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ad8bef52b841b98fe1e79474ca98c1"}},"metadata":{}},{"name":"stdout","text":"Generated embeddings for 1500 captions.\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"# Evaluate cosine similarity on subset\n\nn_eval = min(500, len(text_embeddings_cpu))\n\n# Get subsets and move to device\neval_text_subset = text_embeddings_cpu[:n_eval].to(device)\neval_img_subset = image_embeddings_cpu[:n_eval].to(device)\n\nmlp.eval()\nwith torch.no_grad():\n    with torch.amp.autocast(device_str, dtype=torch.float16):\n        eval_predicted = mlp(eval_text_subset)\n\n    # F.cosine_similarity handles mixed precision (float16 preds, float32 targets)\n    cosine_sim_gpu = F.cosine_similarity(eval_predicted, eval_img_subset, dim=1)\n    cosine_scores = cosine_sim_gpu.cpu().numpy()\n\nprint(f\"Average cosine similarity: {np.mean(cosine_scores):.4f}\")\nprint(f\"Std: {np.std(cosine_scores):.4f}\")\nprint(f\"Range: [{np.min(cosine_scores):.4f}, {np.max(cosine_scores):.4f}]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T11:17:51.512519Z","iopub.execute_input":"2025-11-12T11:17:51.513062Z","iopub.status.idle":"2025-11-12T11:17:51.521931Z","shell.execute_reply.started":"2025-11-12T11:17:51.513041Z","shell.execute_reply":"2025-11-12T11:17:51.521195Z"}},"outputs":[{"name":"stdout","text":"\nRunning sanity check (cosine similarity on train subset)...\nAverage cosine similarity: 0.3817\nStd: 0.0796\nRange: [0.0455, 0.5437]\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"# Create submission CSV\n\nsubmission_data = [\n    {'id': caption_id, 'embedding': predictions[i].tolist()}\n    for i, caption_id in enumerate(caption_ids)\n]\n\nsubmission_df = pd.DataFrame(submission_data)\nsubmission_df.to_csv('submission23.csv', index=False, quoting=0)\n\nprint(f\"\\nâœ… Submission saved with {len(submission_df)} rows.\")\nsubmission_df.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T11:17:55.862058Z","iopub.execute_input":"2025-11-12T11:17:55.862560Z","iopub.status.idle":"2025-11-12T11:17:58.333961Z","shell.execute_reply.started":"2025-11-12T11:17:55.862538Z","shell.execute_reply":"2025-11-12T11:17:58.333388Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Submission saved with 1500 rows.\n","output_type":"stream"},{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"   id                                          embedding\n0   1  [-0.0206146240234375, 0.018218994140625, 0.017...\n1   2  [-0.033355712890625, -0.10308837890625, -0.008...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[-0.0206146240234375, 0.018218994140625, 0.017...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[-0.033355712890625, -0.10308837890625, -0.008...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":105}]}